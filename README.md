# Optimization-of-modern_ml_algos

The folder contains several summaries of the latest optimization techniques being applied in machine learning algorithms.


Links to all papers:


Nestrov. Introductory Lectures on Convex Programming. 1998. (Sections 2.1.1 and 2.1.5)
Bottou et al. Optimization Methods for Large-Scale Machine Learning. Siam 2018. (Sections 2.1 and 2.2)
Stochastic method	
1. Bottou, LÃ©on. Stochastic gradient descent tricks. Neural networks: Tricks of the trade, 2012.
Incremental method
1. Defazio et al. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. NeurIPS 2014.	
Variance reduction	
1. Johnson, Rie and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. NeurIPS 2013.
Momentum-based variance reduction in non-convex sgd. NeurIPS 2020.
Compressed method
1. Bernstein, Jeremy, et al. signSGD: Compressed optimisation for non-convex problems. ICML 2018.
2. Karimireddy, Sai Praneeth, et al. Error feedback fixes signsgd and other gradient compression schemes. ICML 2019.	S5.1
Adaptive optimizer
1. Kingma, Diederik P., and Jimmy Ba. Adam: A method for stochastic optimization. ICLR 2014.
2. Liu, Liyuan, et al. On the variance of the adaptive learning rate and beyond. ICLR 2020.	S6.1
Zeroth-order method
1. Tu, Chun-Chen, et al. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. AAAI 2019.
2. Chen, Pin-Yu, et al. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. ACM workshop on artificial intelligence and security, 2017.	S7.1
Zeroth-order variance reduction
1. Liu, Sijia, et al. Zeroth-order stochastic variance reduction for nonconvex optimization. NeurIPS 2018.
2. Ji, Kaiyi, et al. Improved zeroth-order variance reduced algorithms and analysis for nonconvex optimization. ICML 2019.	S8.1
MAML in meta-learning
1. Finn, Chelsea, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ICML 2017.
2. Finn, Chelsea, et al. Online meta-learning. ICML 2019.	S9.1
MAML with partial adaptation
1. Raghu, Aniruddh, et al. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. ICLR 2020.
2. Ji, Kaiyi, et al. Convergence of meta-learning with task-specific adaptation over partial parameters. NeurIPS 2020	S10.1
Minimax optimizer
1. Liu, Mingrui, et al. Towards better understanding of adaptive gradient algorithms in generative adversarial nets. ICLR 2020.	S11
Bilevel optimizer
1. Franceschi, Luca, et al. Bilevel programming for hyperparameter optimization and meta-learning. ICML 2018.
2. Rajeswaran, et al. Meta-learning with implicit gradients. NeurIPS 2019.
Advanced bilevel method
1. Ji, Kaiyi, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. ICML 2021.
2. Liu, Risheng, et al. A value-function-based interior-point method for non-convex bi-level optimization. ICML 2021.
Hessian-free bilevel method
1. Liu, Hanxiao, et al. Darts: Differentiable architecture search. ICLR 2019.
2. Nichol, Alex, et al. On first-order meta-learning algorithms. 2018.
Federate learning
1. McMahan, Brendan, et al. Communication-efficient learning of deep networks from decentralized data. AISTATS 2017.
2. Li, Tian, et al. Federated optimization in heterogeneous networks. MLSys 2020: 429-450.
